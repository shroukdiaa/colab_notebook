{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2Fugc/KEeRbGBhe3+4ySv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shroukdiaa/colab_notebook/blob/main/NLP_task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hyue40h7xCRt",
        "outputId": "37d1d42e-910f-442b-9843-765f54eca3fd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2"
      ],
      "metadata": {
        "id": "NllnW-ZexELc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1shDW8eRuS0s"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import brown\n",
        "from nltk.tag import UnigramTagger\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('abc')\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdanRpzsuxqc",
        "outputId": "b9e02ad2-1041-4e71-f2be-e682fdec7ad2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]   Package abc is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus.reader import PlaintextCorpusReader"
      ],
      "metadata": {
        "id": "3zoiehHTyktN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Reading Text**"
      ],
      "metadata": {
        "id": "4bfVADC1yy6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = r\"/content/text_NLP.txt\"\n",
        "reader = PlaintextCorpusReader(directory, \".*\\.txt\")\n",
        "reader.fileids()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJDfZQKCykq8",
        "outputId": "246e8d88-b69a-4f41-e1a9-355301cb4457"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = reader.raw(\"/content/text_NLP.txt\")\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "GGnIkvbAykoD",
        "outputId": "2e3dd61f-440f-4eb5-efaf-e54f84d25e17"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Errors cont.\\r\\nIn NLP we are always dealing with these kinds of \\r\\nerrors.\\r\\nReducing the error rate for an application often \\r\\ninvolves two antagonistic efforts: \\r\\n◦ Increasing accuracy or precision (minimizing false \\r\\npositives)\\r\\n◦ Increasing coverage or recall (minimizing false negatives)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = abc.raw(\"/content/text_NLP.txt\")\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iwy-zkfEykil",
        "outputId": "34689b1b-c229-47bf-b5a9-a1a8484a6b40"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Errors cont.\\r\\nIn NLP we are always dealing with these kinds of \\r\\nerrors.\\r\\nReducing the error rate for an application often \\r\\ninvolves two antagonistic efforts: \\r\\n\\xe2\\x97\\xa6 Increasing accuracy or precision (minimizing false \\r\\npositives)\\r\\n\\xe2\\x97\\xa6 Increasing coverage or recall (minimizing false negatives)'"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Part-of-Speech Tagging**"
      ],
      "metadata": {
        "id": "OE91xSrF0wvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **a. Using a Pre-Trained Tagger**"
      ],
      "metadata": {
        "id": "AkyujT-q1Yra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(text.decode('utf-8'))\n",
        "taggings = pos_tag(tokens)\n",
        "taggings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHyJOXfSykgC",
        "outputId": "d2bf3978-5ae9-468c-a338-2a21f844aaed"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Errors', 'NNS'),\n",
              " ('cont', 'VBP'),\n",
              " ('.', '.'),\n",
              " ('In', 'IN'),\n",
              " ('NLP', 'NNP'),\n",
              " ('we', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('always', 'RB'),\n",
              " ('dealing', 'VBG'),\n",
              " ('with', 'IN'),\n",
              " ('these', 'DT'),\n",
              " ('kinds', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('errors', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('Reducing', 'VBG'),\n",
              " ('the', 'DT'),\n",
              " ('error', 'NN'),\n",
              " ('rate', 'NN'),\n",
              " ('for', 'IN'),\n",
              " ('an', 'DT'),\n",
              " ('application', 'NN'),\n",
              " ('often', 'RB'),\n",
              " ('involves', 'VBZ'),\n",
              " ('two', 'CD'),\n",
              " ('antagonistic', 'JJ'),\n",
              " ('efforts', 'NNS'),\n",
              " (':', ':'),\n",
              " ('◦', 'VB'),\n",
              " ('Increasing', 'VBG'),\n",
              " ('accuracy', 'NN'),\n",
              " ('or', 'CC'),\n",
              " ('precision', 'NN'),\n",
              " ('(', '('),\n",
              " ('minimizing', 'VBG'),\n",
              " ('false', 'JJ'),\n",
              " ('positives', 'NNS'),\n",
              " (')', ')'),\n",
              " ('◦', 'VBP'),\n",
              " ('Increasing', 'VBG'),\n",
              " ('coverage', 'NN'),\n",
              " ('or', 'CC'),\n",
              " ('recall', 'NN'),\n",
              " ('(', '('),\n",
              " ('minimizing', 'VBG'),\n",
              " ('false', 'JJ'),\n",
              " ('negatives', 'NNS'),\n",
              " (')', ')')]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **b. Training a Custom Tagger**"
      ],
      "metadata": {
        "id": "VaG8m4QM1nJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_sentences = brown.tagged_sents()[:3000]\n",
        "tagger = UnigramTagger(tagged_sentences[:1500])\n",
        "tagger.accuracy(tagged_sentences[1500:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpOUSvoqykdY",
        "outputId": "b65a4444-0ec8-4dcb-c311-0152b4f0a2d9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7214888552261415"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. lemmatizer**"
      ],
      "metadata": {
        "id": "hwSvA_D15TNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_lemmatization(word_tokens, pos_tagged_sentences=None):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Lemmatization without providing POS tags\n",
        "    # Fixed: Using word_tokens directly for lemmatization without POS\n",
        "    lemmatized_without_pos = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
        "\n",
        "    # Lemmatization with providing POS tags\n",
        "    lemmatized_with_pos = []\n",
        "    if pos_tagged_sentences:\n",
        "        # Fixed: Now iterating over pos_tagged_sentences correctly\n",
        "        for word, tag in pos_tagged_sentences:\n",
        "            wordnet_tag = treebank_to_wordnet_pos(tag)\n",
        "            lemmatized_with_pos.append(lemmatizer.lemmatize(word, wordnet_tag))\n",
        "\n",
        "    return lemmatized_without_pos, lemmatized_with_pos\n",
        "\n",
        "lemmatized_without_pos, lemmatized_with_pos = perform_lemmatization(word_tokens, pos_tagged_sentences)\n",
        "# The following print statements were incorrectly indented\n",
        "print(\"\\nLemmatized Sentences (without POS tags):\\n\", lemmatized_without_pos)\n",
        "print(\"\\nLemmatized Sentences (with POS tags):\\n\", lemmatized_with_pos)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFL62QvPykW2",
        "outputId": "7da1c006-371f-40ef-92f6-fc3890254c6d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatized Sentences (without POS tags):\n",
            " ['This', 'is', 'a', 'sample', 'sentence', '.']\n",
            "\n",
            "Lemmatized Sentences (with POS tags):\n",
            " ['This', 'be', 'a', 'sample', 'sentence', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Perform tokenization (sentence and word)**"
      ],
      "metadata": {
        "id": "9fMiS02c5RwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform tokenization (sentence and word)\n",
        "def tokenize_text(text):\n",
        "    sentences = sent_tokenize(text)  # Sentence tokenization\n",
        "    word_tokens = [word_tokenize(sentence) for sentence in sentences]  # Word tokenization per sentence\n",
        "    return sentences, word_tokens\n",
        "\n",
        "sentences, word_tokens = tokenize_text(text)\n",
        "print(\"\\nTokenized Sentences:\\n\", word_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1DYDCPwykTq",
        "outputId": "29fcbe35-d71e-43df-b7af-1ef28f778648"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized Sentences:\n",
            " [['This', 'is', 'a', 'sample', 'sentence', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.Perform stemming using PorterStemmer**"
      ],
      "metadata": {
        "id": "CR5K21Gq6hUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_stemming(word_tokens):\n",
        "    ps = PorterStemmer()\n",
        "    stemmed_sentences = [[ps.stem(word) for word in sentence] for sentence in word_tokens]\n",
        "    return stemmed_sentences\n",
        "\n",
        "stemmed_sentences = perform_stemming(word_tokens)\n",
        "print(\"\\nStemmed Sentences:\\n\", stemmed_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fQAwLtWykQ2",
        "outputId": "e1a7ce8d-11d8-4d83-d401-4d473438ba0a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemmed Sentences:\n",
            " [['thi', 'is', 'a', 'sampl', 'sentenc', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6.  Perform POS tagging**"
      ],
      "metadata": {
        "id": "UdKW4geW6-a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_pos_tagging(word_tokens):\n",
        "    pos_tagged_sentences = [pos_tag(sentence) for sentence in word_tokens]\n",
        "    return\n",
        "    pos_tagged_sentences\n",
        "\n",
        "pos_tagged_sentences = perform_pos_tagging(word_tokens)\n",
        "print(\"\\nPOS Tagged Sentences:\\n\", pos_tagged_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oevT7j_ykN-",
        "outputId": "90b5b4e1-d10a-466e-89c4-5306adb9c16a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS Tagged Sentences:\n",
            " None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Convert Treebank POS tags to WordNet POS tags for lemmatization**"
      ],
      "metadata": {
        "id": "7cZCAAzc7M4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def treebank_to_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun if not recognized\n",
        "print(\"\\nPOS Tagged Sentences:\\n\", pos_tagged_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0isJMjRykLV",
        "outputId": "e1065f59-e8c6-4c8c-dd5d-4ba0826acb57"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS Tagged Sentences:\n",
            " None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Perform lemmatization (without and with POS tags)**"
      ],
      "metadata": {
        "id": "S-t2hjrl77Ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_lemmatization(word_tokens, pos_tagged_sentences=None):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Lemmatization without providing POS tags\n",
        "    lemmatized_without_pos = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in word_tokens]\n",
        "\n",
        "    # Lemmatization with providing POS tags\n",
        "    lemmatized_with_pos = []\n",
        "    if pos_tagged_sentences:\n",
        "        for sentence in pos_tagged_sentences:\n",
        "            lemmatized_sentence = []\n",
        "            for word, tag in sentence:\n",
        "                wordnet_tag = treebank_to_wordnet_pos(tag)\n",
        "                lemmatized_sentence.append(lemmatizer.lemmatize(word, wordnet_tag))\n",
        "            lemmatized_with_pos.append(lemmatized_sentence)\n",
        "\n",
        "    return lemmatized_without_pos, lemmatized_with_pos\n",
        "    lemmatized_with_pos\n",
        "\n",
        "lemmatized_without_pos, lemmatized_with_pos = perform_lemmatization(word_tokens, pos_tagged_sentences)\n",
        "print(\"\\nLemmatized Sentences (without POS tags):\\n\", lemmatized_without_pos)\n",
        "print(\"\\nLemmatized Sentences (with POS tags):\\n\", lemmatized_with_pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-J2aOShgykId",
        "outputId": "9ae0c396-f805-4ed1-c90c-58aae18f40cb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatized Sentences (without POS tags):\n",
            " [['This', 'is', 'a', 'sample', 'sentence', '.']]\n",
            "\n",
            "Lemmatized Sentences (with POS tags):\n",
            " []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FYw8vGc4ykF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nC9Om61OykDR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}